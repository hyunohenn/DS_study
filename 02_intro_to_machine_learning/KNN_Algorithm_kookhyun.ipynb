{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# KNN\n",
    "\n",
    "![knn_picture](http://i.imgur.com/gLBo1gX.png)\n",
    "\n",
    "- 새로운 데이터가 들어왔을 때, 가장 가까운 k개의 이웃의 정보로 새로운 데이터의 분류(classification)를 예측\n",
    "- __학습__의 개념이 없음, 새로운 데이터가 왔을 때 기존 데이터와의 거리를 계산할 뿐 → `Lazy Model`, `Instance Based Learning`이라고도 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T01:48:38.395178Z",
     "start_time": "2021-04-19T01:48:38.386375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'auto',\n",
       " 'leaf_size': 30,\n",
       " 'metric': 'minkowski',\n",
       " 'metric_params': None,\n",
       " 'n_jobs': None,\n",
       " 'n_neighbors': 5,\n",
       " 'p': 2,\n",
       " 'weights': 'uniform'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNeighborsClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 파라미터들 중 `n_neighnors` , `metric`, `p` 를 살펴볼 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K의 값\n",
    "\n",
    "- 탐색할 데이터의 수 \n",
    "- `n_neighbors` 키워드 사용\n",
    "- K의 값이 너무 작으면 데이터 포인트 하나하나를 따져 Overfitting 발생\n",
    "- K의 값이 너무 크면 과하게 정규화되어 Underfitting 발생\n",
    "\n",
    "![K크기조절](http://i.imgur.com/6Ub8CXe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T05:31:12.784161Z",
     "start_time": "2021-04-19T05:31:12.652334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when K is 1 : Cross Validation Mean: 0.96 // Std: 0.02494\n",
      "when K is 5 : Cross Validation Mean: 0.9733 // Std: 0.02494\n",
      "when K is 10 : Cross Validation Mean: 0.98 // Std: 0.02667\n",
      "when K is 20 : Cross Validation Mean: 0.96 // Std: 0.03266\n",
      "when K is 50 : Cross Validation Mean: 0.9133 // Std: 0.02667\n"
     ]
    }
   ],
   "source": [
    "# 데이터로 확인하기\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn3 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn4 = KNeighborsClassifier(n_neighbors=20)\n",
    "knn5 = KNeighborsClassifier(n_neighbors=50)\n",
    "knns = [knn1, knn2, knn3, knn4, knn5]\n",
    "for i in range(len(knns)):\n",
    "    model = knns[i]\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    print(f'when K is {model.n_neighbors} : Cross Validation Mean: {round(scores.mean(), 4)} // Std: {round(scores.std(), 5)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금은 데이터가 깔끔해서 큰 차이가 보이지는 않지만, 정확성이 높고 분산이 작은 5나 10이 좋아 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance의 정의\n",
    "\n",
    "- 어떤 거리 개념을 사용할 것인지\n",
    "- `metric`, `p` 키워드를 통해 설정\n",
    "-  기본값은 `metric='minkowski'`, `p=2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __참고 : 다양한 거리의 개념__\n",
    "$$\n",
    "\\text{Manhattan Distance : } \\sum_1^n{|x_i - y_i|} \\\\\n",
    "\\text{Euclidean Distance : } \\sqrt{\\sum_1^n{(x_i - y_i)^2}} \\\\\n",
    "\\text{일반화 => Minkowski Distance : } {\\left( \\sum_1^n{|x_i - y_i|^p} \\right)}^\\frac{1}{p} \\\\\n",
    "$$\n",
    "\n",
    "> __참고2 : 공분산 고려 거리__\n",
    "$$\n",
    "\\text{Mahalanobis Distance : } \\sqrt{(\\vec{X} - \\vec{Y})\\Sigma^{-1}(\\vec{X} - \\vec{Y})} \\\\\n",
    "\\Sigma \\text{ is covariance matrix} \\\\\n",
    "$$\n",
    "- Mahalanobis Distance 에서 p의 값을 파라미터로 사용할 수도 있음\n",
    "- `metric` 인자를 __wminkovski__로 설정하여 가중치 사용할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T06:11:43.493406Z",
     "start_time": "2021-04-19T06:11:43.395167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when metric is minkowski : Cross Validation Mean: 0.9733 // Std: 0.02494\n",
      "when metric is euclidean : Cross Validation Mean: 0.9733 // Std: 0.02494\n",
      "when metric is manhattan : Cross Validation Mean: 0.96 // Std: 0.03266\n"
     ]
    }
   ],
   "source": [
    "# 데이터로 확인하기\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "knn1 = KNeighborsClassifier(p=3)\n",
    "knn2 = KNeighborsClassifier(metric='euclidean')\n",
    "knn3 = KNeighborsClassifier(metric='manhattan')\n",
    "\n",
    "\n",
    "knns = [knn1, knn2, knn3]\n",
    "for i in range(len(knns)):\n",
    "    model = knns[i]\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    print(f'when metric is {model.metric} : Cross Validation Mean: {round(scores.mean(), 4)} // Std: {round(scores.std(), 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주의점 및 단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주의점\n",
    "1. 불균형 데이터에 대해 __사전 확률(prior probability)__ 고려 필요  \n",
    "||정상|불량|\n",
    "|---|---|---|\n",
    "|사전확률|0.99|0.01|\n",
    "|주변K개|0.8|0.2|\n",
    "\n",
    "- 위와 같이 새로운 데이터의 주변 k개의 분포가 정상이 더 높아도, 분류는 불량으로 해야 하는 것이 맞는 상황 발생\n",
    "- `weights` 인자에 함수를 할당하여 처리 가능\n",
    "\n",
    "2. __정규화__ 필요  \n",
    "||인구|위도|경도|\n",
    "|---|---|---|---|\n",
    "|서울|1000만|37|126|\n",
    "|뉴욕|840만|43|75|\n",
    "\n",
    "- 위와 같은 경우 거리를 계산할 때 인구의 영향이 너무 큼\n",
    "- 따라서 모든 변수의 영향력이 비슷하도록 정규화 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단점\n",
    "- 새로운 관측치와 기존의 관측치 간의 거리를 모두 계산해야 하므로 __계산 복잡도__가 높음\n",
    "- __차원의 저주(Curse of Dimensionality)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 참조: [ratsgo's blog](https://ratsgo.github.io/machine%20learning/2017/04/17/KNN/), [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
